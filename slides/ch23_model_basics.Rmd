---
title: "Model basics"
author: "Tomoya Fukumoto"
date: "2023-10-10"
output:
  ioslides_presentation:
    widescreen: true 
    css: style/style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Model basics
モデルを作る基本となる理論
 
### 準備
```{r prep, message=FALSE}
library(tidyverse)
library(modelr)
options(na.action = na.warn)
```

> モデルの目的は真実を明らかにすることではない<br>
> 役に立つ近似を見つけることだ

## 23.1 Introduction

## モデルを作る手順
### Step1 Model Familyを決める
- 直線 or 二次曲線 or etc.
- 説明変数
- データが持つパターンを捕らえる

### Step2 Model Fit
- Model Familyのうち最も良いモデルを採用する
- 良さ ⇒ 訓練データを使った予測結果と実際が一番近い
- 近さ ⇒ 基準を決める(e.g. 二乗誤差)
- 機械学習ではtrainingともいう

## 23.2 A simple model
単純なモデルを使って直線モデル当てはめの基本

## テストデータ
```{r sim1, fig.height=3.5}
ggplot(sim1, aes(x, y)) + geom_point()
```

直線的な関係がありそうだ
$$
y=a_0+a_1x
$$

## 250個のテキトーな直線
```{r sim1_random_model, fig.height=3.3}
models <- tibble(a1 = runif(250, -20, 40), a2 = runif(250, -5, 5))
ggplot(sim1, aes(x, y)) +
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

このうちどれが良いモデルだろうか？

## モデルによる予測
```{r sim1_pred}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}
model1(c(7, 1.5), sim1)
```
- `a[1]`: 切片
- `a[2]`: 傾き
- `data`: data.frameでx列が説明変数の実データ
 
## モデル予測の誤差
```{r sim1_error}
## RMSE
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^2))
}
measure_distance(c(7, 1.5), sim1) 
```
$$
\mathrm{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^N \left(y_i - (a_0 + a_1 x)\right)^2}
$$

## 全モデルの予測誤差
```{r sim1_allerror}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}
models <- models %>% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

## top10モデル
```{r sim1_10bestmodel, fig.height=3.8}
ggplot(sim1, aes(x, y)) + geom_point(size = 2, color = "grey30") +
  geom_abline(
    data = filter(models, rank(dist) <= 10),
    aes(intercept = a1, slope = a2, color = -dist)
  )
```

## 良いパラメータはどれ？
```{r sim1_modelparam}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, color = "red") +
  geom_point(aes(color = -dist))
```

## 誤差の最適化
```{r sim1_bestmodel, fig.height=3}
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par
ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "gray30") +
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

## 線形モデルを使った方法
```{r sim1_lm}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

### 実は線形モデルは解析的に解ける
$$\begin{eqnarray}
\hat a_0 &=& \bar{y} - \hat a_1\bar{x}\\
\hat a_1 &=& \frac{\sum_{i=1}^N(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^N(x_i-\bar x)^2}
\end{eqnarray}$$

## 23.2.1 Exercises

## 23.3 Visualising models
作ったモデルを理解したい
  
- 予測値: モデルが捉えたもの
- 残差: モデルが捉えなかったもの

線形モデルに限らずすべての予測モデルで使える考え方

## 23.3.1 Prediction
モデルが捉えたもの

## `modelr::data_grid()`
### Usage
```{r usage.datagrid, eval=FALSE}
data_grid(data, ..., .model = NULL)
```

### Arguments
||
|---|---|
| **data** | data.frame |
| **...** | `tidyr::expand()` に渡すパラメータ|
| **.model** | もし設定された場合、モデルに使われている変数も出力する

### Value
`data` の指定された変数について実データの値の取る範囲に基づいてグリッドを作る。
作られたグリッドがデータフレームとして返ってくる

## 下準備 {.columns-2}
 
```{r sim1_grid}
grid <- sim1 %>% data_grid(x)
grid
```

```{r sim1_remember}
# Remember
sim1 
```


## `modelr::add_predictions()`
### Usage
```{r usage.addpred, eval=FALSE}
add_predictions(data, model, var = "pred", type = NULL)
```

### Arguments
||
|---|---|
| **data** | 予測をする対象になるデータ(data.frame) |
| **model** | 予測を行うモデル |
| **var** | 予測を出力する列名 |
| **type** | 予測タイプ。通常は自動設定される |

### Value
`data` で定められたdata.frameに予測列を追加したdata.frame

## 予測を得る 
```{r sim1_getpred}
# sim1_mod <- lm(y~x, data = sim1)
grid <- grid %>% 
  add_predictions(sim1_mod)
grid
```

## 予測能力を可視化
```{r sim1_predplot}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

## 23.3.2 Residuals
モデルが捉えなかったもの

## `modelr::add_residuals`
### Usage
```{r usage.addresid, eval=FALSE}
add_residuals(data, model, var = "resid")
```

### Arguments
||
|---|---|
| **data** | 予測する対象になるデータ(data.frame) |
| **model** | 予測を行うモデル |
| **var** | 予測を出力する列名 |

### Value
`data` で定められたdata.frameに予測誤差を追加したdata.frame

## 残差を得る
```{r sim1_getresid}
sim1 <- sim1 %>%
  add_predictions(sim1_mod) %>%  #Remember
  add_residuals(sim1_mod)
sim1
```

## 残差の可視化１ (誤差分布)
```{r sim1_vizresid1}
ggplot(sim1, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)
```
 
## 残差の可視化２ (vs説明変数)
```{r sim1_vizresid2}
ggplot(sim1, aes(x, resid)) +
  geom_ref_line(h = 0) +
  geom_point()
```

## 23.3.3 Exercises

## 23.4 Formulas and model families
Formulaを使ってモデル族(Model family)を設定する

### モデル族 = 統計モデル + モデル式(Formula)
- 統計モデルの例： 線形モデル、ニューラルネットワーク
- モデル式：要するに説明変数
- 機械学習の分野ではモデル式の設定を **特長量エンジニアリング** と言ったりする

### 目次
- 基本の説明変数
- 離散値の説明変数
- 相互作用
- 変数の変換
 
## Model Formula
Rの多くのモデル関数ではFormulaという文法でモデル式を設定できる

### 文法： 目的変数 ~ 説明変数の構造

### 例
線形モデルのモデル関数 `lm` で Formula `y ~ x` を指定する
```{r lm_formula, eval=FALSE}
lm(formula = y~x, data = sim1)
```

これは $y$ を $a_0+a_1 x$ というモデル族を使って予測するということ 


## Model Formulaの調整
|formula|linear model|説明|
|---|---|---|
|`y~x`|$y\sim a_0+a_1x$|標準的な線形回帰|
|`y~x-1`|$y\sim a_1x$|切片ゼロ。比例関係|
|`y~x1+x2`|$y\sim a_0+a_1x_1+a_2x_2$|二変数。いわゆる重回帰|

## 21.4.1 Categorical variables
カテゴリー変数（質的変数）で回帰分析する
 
## 文字列変数を説明変数にFormulaが書ける(便利)
```{r categorical_formula}
df <- tribble(
  ~sex, ~response,
  "male", 1,
  "female", 2,
  "male", 1
)
model_matrix(df, response ~ sex)
```

sexfemale列は無くていいのか？


## 数式で表現すると

$$
y \sim a_0 + a_1 x 
$$
where
$$
x = \left\{
\begin{array}{ll} 1 & (\mathrm{sex = male})\\ 0& (\mathrm{sex = female}) \end{array} \right.
$$

### もしくは
$$
y \sim \left\{ \begin{array}{ll} a_0 + a_1 & (\mathrm{sex} = \mathrm{male})\\ a_0       & (\mathrm{sex} = \mathrm{female})\end{array}\right.
$$

## 言葉で表現すると
- $a_0$は`sex=female`の`y`の平均値
- $a_1$は`sex=male`の`y`の平均値と`sex=female`の`y`の平均値$a_0$との差

### 数式再掲
$$
y \sim \left\{ \begin{array}{ll} a_0 + a_1 & (\mathrm{sex} = \mathrm{male})\\ a_0       & (\mathrm{sex} = \mathrm{female})\end{array}\right.
$$


## お題 {.columns-2}
```{r sim2_plot}
sim2
mod2 <- lm(y ~ x, data = sim2)
grid <- sim2 %>%
  data_grid(x) %>%
  add_predictions(mod2)
grid
```

## 分析
```{r sim2_plotpred}
ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), color = "red", size = 4)
```

## 23.4.2 交互作用(連続-カテゴリ)
説明変数２つ $x_1$(連続) と $x_2$(カテゴリ) で回帰分析するとき、２変数にシナジーがある

### Model Formula
```{r interaction_concat, eval=FALSE}
y ~ x1 * x2
```

### 数式(二値の場合)
$$
y \sim \left\{
 \begin{array}{ll}
  a_0 + a_1 x_1 + a_21 + a_3 x_1 1  &(x_2 = \mathrm{value1})\\ 
  a_0 + a_1 x_1 + a_2 0 + a_3 x_1 0 & (x_2 = \mathrm{value2})
 \end{array} \right.
$$

## お題
```{r sim3_plot}
ggplot(sim3, aes(x1, y)) +
  geom_point(aes(color = x2))
```

## 分析
```{r sim3_lm}
mod1 <- lm(y ~ x1 + x2, data = sim3) #交互作用無しの回帰分析
mod2 <- lm(y ~ x1 * x2, data = sim3) #交互作用有りの回帰分析
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
grid
```

## モデルの可視化
```{r sim3_model_plot, fig.height=4}
ggplot(sim3, aes(x1, y, color = x2)) +
  geom_point() +
  geom_line(data = grid, aes(y = pred)) +
  facet_wrap(~ model)
```

## 残差の可視化
```{r sim3_model_residplot, fig.height=3.8}
sim3 %>% 
  gather_residuals(mod1, mod2) %>% 
  ggplot(aes(x1, resid, color = x2)) +
  geom_point() +
  facet_grid(model ~ x2)
```

## 23.4.3 交互作用（連続-連続)
説明変数２つ $x_1$(連続), $x_2$(連続) で回帰分析するとき、２変数にシナジーがある

### Model formula
```{r interaction_conccon, eval=FALSE}
y ~ x1 * x2
```

### 数式
$$
y \sim a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1x_2
$$

## お題 {.columns-2}
```{r sim4_viz, fig.width=5, fig.height=4}
sim4
ggplot(sim4, aes(x1, x2)) +
  geom_tile(aes(fill = y))
```

## 分析 
```{r sim4_analysis}
mod1 <- lm(y ~ x1 + x2, data = sim4) #交互作用なし
mod2 <- lm(y ~ x1 * x2, data = sim4) #交互作用あり
grid <- sim4 %>% 
  data_grid( x1 = seq_range(x1, 5), x2 = seq_range(x2, 5) ) %>% 
  gather_predictions(mod1, mod2)
grid
```

## モデルの可視化{.columns-2}
```{r sim4_modelviz1, fig.width=5, fig.height=4}
ggplot(grid, aes(x1, pred)) +
  geom_line(aes(color = x2, group = x2)) + 
  facet_wrap(~ model)
ggplot(grid, aes(x2, pred)) +
  geom_line(aes(color = x1, group = x1)) +
   facet_wrap(~ model)
```

## 残差の可視化{.columns-2}
```{r sim4_residviz, message=FALSE, fig.width=5, fig.height=4}
sim4 <- gather_residuals(sim4, mod1, mod2) 
ggplot(sim4, aes(x = resid))  +
  geom_freqpoly(aes(color = model))
ggplot(sim4, aes(x1, x2))  +
  geom_tile(aes(fill = resid)) +
  facet_wrap(~model)
```

## モデルの統計的な指標たち
```{r sim4_model_summary}
summary(mod2)
```

## 23.4.4 変換
説明変数や目的変数を初等関数を使って変換する

### Model formula
```{r transformation}
I(y) ~ I(x)
```
   
## 具体例
|formula|model|説明|
|---|---|---|
|`y~I(x^2)`|$y\sim a_0+a_1x^2$|二乗を使った回帰|
|`y~poly(x, 3)`|$y\sim a_0+a_1+a_2x^2+a_3x^3$|三次回帰|
|`I(log(y))~I(log(x))`|$\log y\sim a_0 + a_1 \log x$|対数変換。めちゃくちゃ重要|

## いろいろと理論的なよもやま話

- 多項式モデル `poly(x, p)` を使って、次数 $p$ を無限大にすると任意の曲線を近似できるテイラーの定理）
    - 多項式の次元を大きくすると $x$ が極端に大きいときは急速に大きく/小さく なってうまくいかない
    - またモデルの自由度が高すぎると過学習(overfitting)して予測はうまくいかない
- ナイスな代案はスプラインを使うこと、特に **自然スプライン** は有益だ
- 自然スプラインとは直線、二次曲線、三次曲線をいい感じに繋いで一つのグラフを作る方法
- `ns(x, p)` という関数を使えば自然スプラインのFormulaが自動的に書けちゃう。ただし $p$ は自由度

## 23.4.5 Exercises

## 23.5 欠損値

- 欠損値はモデル学習には利用できない
- 選択肢
    - 欠損値のある行をすべて削除してしまう
    - 欠損値を何かの値で代用する（ゼロとか）
    - 欠損値を平均値などで埋める（高級なテクニック）
    
非常にテクニカルなので詳細は立ち入らない

## 23.6 その他のモデル

- 一般化線形モデル `stats::glm()`
- 一般化加法モデル `mgcv::gam()`
- ペナルティ付き線形モデル（正則化） `glmnet::glmnet()`
- ロバスト線形回帰 `MASS::rlm()`
- 決定木 `rpart::rpart()`
    - ランダムフォレスト `randomForest::randomForest()`
    - 勾配ブースティング `xgboost::xgboost()`